# -*- coding: utf-8 -*-
"""I078,I036,I042AIProject_predict_movie_genre_from_plot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pYAJgG-_-bbfTDQ7PtPTyVPb23hbsUCl

# AI PROJECT

#Movie Genre Prediction using DL Models
Application of Naive Bayes, Logistic Regression, Decision Tree, Stochastic Gradient Descent on movie plots to correctly predict its genre.

# Dataset used

The dataset has 34,886 entries of movie plot summaries. Information found in the dataset is as follows:

1. Release Year - year of release
2. Title - title of the movies
3. Origin/Ethnicity - country of origin of the movies
4. Director - director names associated with the movies
5. Cast - cast name associated with the movies
6. Wiki Page - wikipedia page of the movies
7. Plot - plot summary of the movies

# Data Exploration

## Import necessary libraries and packages
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import itertools
import warnings
import seaborn as sns
# from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import GaussianNB
import sklearn.metrics
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix
warnings.filterwarnings('ignore')
import re
import nltk
import string
from nltk.corpus import stopwords
from nltk import word_tokenize, FreqDist
from nltk.stem import WordNetLemmatizer
import spacy
from wordcloud import WordCloud, STOPWORDS
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from yellowbrick.text import FreqDistVisualizer
# import dataframe_image as dfi
# from mlxtend.feature_selection import ColumnSelector

import spacy.cli
spacy.cli.download("en_core_web_lg")
nlp = spacy.load("en_core_web_lg")

"""## Read data into pandas"""

wiki= pd.read_csv('/content/wiki_movie_plots_deduped.csv')

wiki.head()

"""## Descriptive Statistics

### The release years range from 1901 to 2017 in the dataset
"""

wiki.describe()

"""### There are 34886 rows and 8 columns """

wiki.shape

wiki.info()

"""# Exploratory Data Analysis

## Movie Distributions by Release Year
"""

fig, ax = plt.subplots(figsize=(10,6))

sns.histplot(x = wiki['Release Year'])
plt.title('Frequency of Release Years')
plt.xlabel('Realease Year')
plt.ylabel('Frequency of Occurence')
fig.savefig('./freqofreleaseyear.png');

"""## Movie distribution by Movie Origin"""

fig, ax = plt.subplots(figsize=(10,6))

sns.countplot(x = wiki['Origin/Ethnicity'])
plt.gcf().subplots_adjust(bottom=0.15)
plt.xticks(rotation = 90)
plt.title('Distribution of Movie Origin')
plt.xlabel('Movie Origin')
plt.ylabel('Frequency Distribution')
fig.savefig('./FreqDistOrigin.png');

"""## Subsetting the dataset to include only the entries with a known genre and resetting the index"""

wiki_df = wiki.loc[wiki['Genre']!='unknown']
wiki_df.reset_index(drop = True, inplace = True)

"""Checking the number of unique values we have in the genres column"""

genres=pd.value_counts(wiki_df.Genre)

print('There are ',len(genres), 'different Genres in the dataset:')
print('-'*50)
print(genres)

"""## Getting the top 6 most commonly occuring genres in the dataset"""

top_genres = pd.DataFrame(genres[:6]).reset_index()
top_genres.columns = ['genres', 'number_of_movies']
top_genres

"""## Movie distribution of the top six genres in the dataset"""

fig, ax = plt.subplots(figsize=(14,8))

sns.barplot( top_genres['number_of_movies'])
plt.title('Top 6 genres and their frequency')
plt.xlabel('genres')
plt.ylabel('frequency')
fig.savefig('./freqoftopgenres.png')

"""## Creating separate labels column for the top 6 genres only and assigning the values between 1-6 for drama, comedy,horror, action, thriller and romance, respectively if present in the genre column of the movie (for entries that have only one genre)"""

conditions = [wiki_df['Genre']=='drama', wiki_df['Genre']=='comedy', wiki_df['Genre']=='horror', wiki_df['Genre']=='action', wiki_df['Genre']=='thriller',
             wiki_df['Genre']=='romance']
choices = [1,2,3,4,5,6]
wiki_df['labels'] = np.select(conditions, choices, 0)

wiki_df.sample(3)

wiki_df['labels'].value_counts()

"""## Subsetting the wiki_df dataframe to only include the rows that have entries (1-6) in the newly created labels columns (the ones that have only a single genre entry)"""

df_to_use = (wiki_df.loc[wiki_df['labels']!=0]).reset_index(drop = True)

# We are now left with about 14 thousand rows of data
df_to_use.head()

"""## Table showing Genre label assignments"""

assigned_label = pd.DataFrame(sorted(list(zip(df_to_use['Genre'].unique(), df_to_use['labels'].unique())), key = lambda x : x[1], reverse = False), columns = ['genres', 'labels'])
pd.merge(assigned_label, top_genres)

# df_to_use.groupby(['Release Year', 'Genre']).agg('count')
fig, ax = plt.subplots(figsize=(10,6))

sns.histplot(x = 'Release Year', hue = 'Genre', data = df_to_use, multiple = 'stack')
plt.title('Frequency of Release Years Showing Genres')
plt.xlabel('Release Year')
plt.ylabel('Frequency of Occurence')
fig.savefig('./freqofreleaseyearandgenres.png');

"""## Dropping unnecessary columns"""

df_to_use.drop(columns = ['Release Year', 'Origin/Ethnicity', 'Director', 'Cast', 'Wiki Page'], axis = 1, inplace = True)

df_to_use.head(2)

df_to_use.describe()

"""## Cleaning up the 'Plot' column for Analysis

Below we are just looking at a sample of the plot summary
"""

df_to_use['Plot'][5]

"""### Total number of words in the Plot summaries"""

def word_length(text):
    num_of_words = 0
    for row in text:
        words = [word for word in row.split(' ')]
        num_of_words += len(words)
    return num_of_words

raw_length = word_length(df_to_use['Plot'])

"""### Total number of UNIQUE words in the plot summaries"""

def unique_word_length(text):
    unique_words = set()
    for row in text:
        words = [word for word in row.split(' ')]
        unique_words.update(words)
    return len(unique_words)

raw_unique_length = unique_word_length(df_to_use['Plot'])

"""### Creating our stopwords list and adding some more words that are very common in the summaries. (I ran the freqdist after this and noticed that some of these words were very common but didn't lend us too much meaning so I came back here to add them to the stopwords list). Changing the text to lower case, stopwords removal, lemmatizing."""

nltk.download('stopwords')

stopwords_list = stopwords.words('english')
stopwords_list += list(string.punctuation)
stopwords_list += ['one', 'two', 'go','goes', 'get', 'also', 'however', 'tells'] 
stopwords_list += [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

def clean_text(text):
    text = text.lower()
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "can not ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r"\'scuse", " excuse ", text)
    text = re.sub('\W', ' ', text)
    text = re.sub('\s+', ' ', text)
    text = re.sub('\d+', ' ', text)
    text = text.strip(' ')
    text = word_tokenize(text)
    text = ' '.join(text)
    text = nlp(text)
    text = [w.lemma_ for w in text]
    text = [w for w in text if w not in stopwords_list]
    text = ' '.join(text)
    return text

"""Rechecking the summary we looked at above to see if our function worked"""

new_df = df_to_use.copy(deep = True)

nltk.download('punkt')

new_df['Plot'] = new_df['Plot'].map(lambda x : clean_text(x))
new_df['Plot'][5]

def word_length(text):
    num_of_words = 0
    for row in text:
        words = [word for word in row.split(' ')]
        num_of_words += len(words)
    return num_of_words

clean_length = word_length(new_df['Plot'])

def unique_word_length(text):
    unique_words = set()
    for row in text:
        words = [word for word in row.split(' ')]
        unique_words.update(words)
    return len(unique_words)

clean_unique_length = unique_word_length(new_df['Plot'])

plot_length = [[raw_length, clean_length], [raw_unique_length, clean_unique_length]]
length_df = pd.DataFrame(plot_length, columns = ['num_of_words', 'num_of_unique_words'])
length_df['index'] = ['raw_plot', 'cleaned_plot']

y1 = length_df['num_of_words']
y2 = length_df['num_of_unique_words']

plt.figure(figsize=(10,7))
ind = np.arange(2)

width = 0.3       

# Plotting
plt.bar(ind, y1, width, label='raw_plot')
plt.bar(ind + width, y2, width, label='cleaned_plot')

plt.xlabel('plot', size = 13)
plt.ylabel('word_length', size = 13)
plt.suptitle('Comparison of total and unique number of words before and after processing', size = 20)
plt.xticks(ind + width / 2, ('words', 'unique_words'))
plt.legend(loc='best')
plt.show()
fig.savefig('./totalvsuniquewords.png')

length_df

"""## More EDA

### Distribution of words in plot and movie title for each genre

DRAMA

Frequency Distribution plot showing the 20 most common words in the plot summaries in drama

We can see from the plot that the most common words have to do with family, life, love, etc
"""

from sklearn.feature_extraction.text import CountVectorizer
count = CountVectorizer()

drama_plot = new_df.loc[new_df['labels'] == 1, ['Plot', 'Title']]
drama_plotlist = [x for x in drama_plot['Plot'].str.split()]
drama_plotlist = list(itertools.chain(*drama_plotlist))

count = CountVectorizer()
docs       = count.fit_transform(drama_plotlist)
features   = count.get_feature_names()


fig = plt.figure(figsize=(10, 10))
plt.suptitle('Drama : Frequency Distribution of Top 10 Words in Plot Summary', size = 20)
plt.yticks(fontsize = 25)
plt.xticks(fontsize = 20)
plt.gcf().subplots_adjust(left=0.15)

visualizer = FreqDistVisualizer(features = features, n=10, orient='h')
visualizer.fit(docs)
visualizer.show()
fig.savefig('./images/freqdistofwordsinplotsummaryfordramagenre.png');

"""The wordcloud below shows the 20 most frequent words in drama

A worldcloud showing the most frequent words in movie titles for drama
"""

plot_corpus = ' '.join(drama_plot_joined)
plot_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', max_words = 20, height=2000, width=4000, random_state = 42, 
                           normalize_plurals = False).generate(plot_corpus)

# Plot the wordcloud
fig, ax = plt.subplots(figsize=(16,8))
plt.imshow(plot_wordcloud)
plt.axis('off')
plt.title('Most Frequent Words in the Movie Plot For Drama', fontsize = 20)
plt.show()
fig.savefig('./images/MoviePlotCloudDrama.png');

title_corpus = ' '.join(drama_plot_joined)
title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', height=2000, width=4000, random_state = 42, max_words = 100).generate(title_corpus)

# Plot the wordcloud
fig, ax = plt.subplots(figsize=(16,8))
plt.imshow(title_wordcloud)
plt.axis('off')
plt.title('Most Frequent Words in the Movie Titles For Drama', fontsize = 20)
plt.show()
fig.savefig('./images/MovieTitleCloudDrama.png');

"""COMEDY

Frequency Distribution plot showing the 20 most common words in the plot summaries in comedy
"""

comedy_plot = new_df.loc[new_df['labels'] == 2, ['Plot', 'Title']]
comedy_plotlist = [x for x in comedy_plot['Plot'].str.split()]
comedy_plotlist = list(itertools.chain(*comedy_plotlist))


count = CountVectorizer()
docs       = count.fit_transform(comedy_plotlist)
features   = count.get_feature_names()


fig = plt.figure(figsize=(10, 10))
plt.suptitle('Comedy : Frequency Distribution of Top 10 Words in Plot Summary', size = 20)
plt.yticks(fontsize = 25)
plt.xticks(fontsize = 20)
plt.gcf().subplots_adjust(left=0.15)

visualizer = FreqDistVisualizer(features = features, n=10, orient='h')
visualizer.fit(docs)
visualizer.show()
fig.savefig('./images/freqdistofwordsinplotsummaryforcomedygenre.png');

"""The wordcloud below shows the 20 most frequent words above for comedy"""

plot_corpus = ' '.join(comedy_plotlist)
plot_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', max_words = 20, height=2000, width=4000, random_state = 42, 
                          normalize_plurals = False).generate(plot_corpus)

# Plot the wordcloud
fig, ax = plt.subplots(figsize=(16,8))
plt.imshow(plot_wordcloud)
plt.axis('off')
plt.title('Most Frequent Words in the Movie Plot For Comedy', fontsize = 20)
plt.show()
fig.savefig('./images/MoviePlotCloudComedy.png');

"""A worldcloud showing the most frequent words in movie titles for comedy"""

title_corpus = ' '.join(comedy_plotlist)
title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', height=2000, width=4000, random_state = 42, max_words = 100).generate(title_corpus)

# Plot the wordcloud
fig, ax = plt.subplots(figsize=(16,8))
plt.imshow(title_wordcloud)
plt.axis('off')
plt.title('Most Frequent Words in the Movie Titles For Comedy', fontsize = 20)
plt.show()
fig.savefig('./images/MovieTitleCloudComedy.png');

"""HORROR

Frequency Distribution plot showing the 20 most common words in the plot summaries in horror. Words like kill, deadly and body appear to be very common
"""

horror_plot = new_df.loc[new_df['labels'] == 3, ['Plot','Title']]
horror_plotlist = [x for x in horror_plot['Plot'].str.split()]
horror_plotlist = list(itertools.chain(*horror_plotlist))

count = CountVectorizer()
docs       = count.fit_transform(horror_plotlist)
features   = count.get_feature_names()


fig = plt.figure(figsize=(10, 10))
plt.suptitle('Horror : Frequency Distribution of Top 10 Words in Plot Summary', size = 20)
plt.yticks(fontsize = 25)
plt.xticks(fontsize = 20)
plt.gcf().subplots_adjust(left=0.15)

visualizer = FreqDistVisualizer(features = features, n=10, orient='h')
visualizer.fit(docs)
visualizer.show()
fig.savefig('./images/freqdistofwordsinplotsummaryforhorrorgenre.png');

"""The wordcloud below shows the same thing above for horror"""

plot_corpus = ' '.join(horror_plot_joined)
plot_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', max_words=20, height=2000, width=4000, random_state = 42, 
                           normalize_plurals = False).generate(plot_corpus)

# Plot the wordcloud
fig, ax = plt.subplots(figsize=(16,8))
plt.imshow(plot_wordcloud)
plt.axis('off')
plt.title('Most Frequent Words in the Movie Plot For Horror', fontsize = 20)
plt.show()
fig.savefig('./images/MoviePlotCloudHorror.png');

"""A worldcloud showing the most frequent words in movie titles for horror"""

title_corpus = ' '.join(horror_plot_joined)
title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', height=2000, width=4000, random_state = 42, max_words = 100).generate(title_corpus)

# Plot the wordcloud
fig, ax = plt.subplots(figsize=(16,8))
plt.imshow(title_wordcloud)
plt.axis('off')
plt.title('Most Frequent Words in the Movie Titles For Horror', fontsize = 20)
plt.show()
fig.savefig('./images/MovieTitleCloudHorror.png');

"""ACTION

Frequency Distribution plot showing the 20 most common words in the plot summaries in action
"""

action_plot = new_df.loc[new_df['labels'] == 4, ['Plot', 'Title']]
action_plotlist = [x for x in action_plot['Plot'].str.split()]
action_plotlist = list(itertools.chain(*action_plotlist))

count = CountVectorizer()
docs       = count.fit_transform(action_plotlist)
features   = count.get_feature_names()


fig = plt.figure(figsize=(10, 10))
plt.suptitle('Action : Frequency Distribution of Top 10 Words in Plot Summary', size = 20)
plt.yticks(fontsize = 25)
plt.xticks(fontsize = 20)
plt.gcf().subplots_adjust(left=0.15)

visualizer = FreqDistVisualizer(features = features, n=10, orient='h')
visualizer.fit(docs)
visualizer.show()
fig.savefig('./images/freqdistofwordsinplotsummaryforactiongenre.png');

"""The wordcloud below shows the same thing above for action"""

plot_corpus = ' '.join(action_plotlist)
plot_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', max_words=20, height=2000, width=4000, random_state = 42, 
                           normalize_plurals = False).generate(plot_corpus)

# Plot the wordcloud
fig, ax = plt.subplots(figsize=(16,8))
plt.imshow(plot_wordcloud)
plt.axis('off')
plt.title('Most Frequent Words in the Movie Plot For Action', fontsize = 20)
plt.show()
fig.savefig('./images/MoviePlotCloudAction.png');

"""A worldcloud showing the most frequent words in movie titles for action"""

title_corpus = ' '.join(action_plotlist)
title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', height=2000, width=4000, random_state = 42, max_words = 100).generate(title_corpus)

# Plot the wordcloud
fig, ax = plt.subplots(figsize=(16,8))
plt.imshow(title_wordcloud)
plt.axis('off')
plt.title('Most Frequent Words in the Movie Titles For Action', fontsize = 20)
plt.show()
fig.savefig('./images/MovieTitleCloudAction.png');

"""THRILLER

Frequency Distribution plot showing the 20 most common words in the plot summaries in thriller
"""

thriller_plot = new_df.loc[new_df['labels'] == 5, ['Plot', 'Title']]
thriller_plotlist = [x for x in thriller_plot['Plot'].str.split()]
thriller_plotlist = list(itertools.chain(*thriller_plotlist))

count = CountVectorizer()
docs       = count.fit_transform(thriller_plotlist)
features   = count.get_feature_names()


fig = plt.figure(figsize=(10, 10))
plt.suptitle('Thriller : Frequency Distribution of Top 10 Words in Plot Summary', size = 20)
plt.yticks(fontsize = 25)
plt.xticks(fontsize = 20)
plt.gcf().subplots_adjust(left=0.15)

visualizer = FreqDistVisualizer(features = features, n=10, orient='h')
visualizer.fit(docs)
visualizer.show()
fig.savefig('./images/freqdistofwordsinplotsummaryforthrillergenre.png');

"""The wordcloud below shows the same thing above for thriller"""

plot_corpus = ' '.join(thriller_plotlist)
plot_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', max_words=20, height=2000, width=4000, random_state = 42, 
                           normalize_plurals = False).generate(plot_corpus)

# Plot the wordcloud
fig, ax = plt.subplots(figsize=(16,8))
plt.imshow(plot_wordcloud)
plt.axis('off')
plt.title('Most Frequent Words in the Movie Plot For Thriller', fontsize = 20)
plt.show()
fig.savefig('./images/MoviePlotCloudThriller.png');

"""A worldcloud showing the most frequent words in movie titles for thriller"""

title_corpus = ' '.join(thriller_plotlist)
title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', height=2000, width=4000, random_state = 42, max_words = 100).generate(title_corpus)

# Plot the wordcloud
fig, ax = plt.subplots(figsize=(16,8))
plt.imshow(title_wordcloud)
plt.axis('off')
plt.title('Most Frequent Words in the Movie Titles For Thriller', fontsize = 20)
plt.show()
fig.savefig('./images/MovieTitleCloudThriller.png');

"""ROMANCE

Frequency Distribution plot showing the 20 most common words in the plot summaries in romance
"""

romance_plot = new_df.loc[new_df['labels'] == 6, ['Plot', 'Title']]
romance_plotlist = [x for x in romance_plot['Plot'].str.split()]
romance_plotlist = list(itertools.chain(*romance_plotlist))

count = CountVectorizer()
docs       = count.fit_transform(romance_plotlist)
features   = count.get_feature_names()


fig = plt.figure(figsize=(10, 10))
plt.suptitle('Romance : Frequency Distribution of Top 10 Words in Plot Summary', size = 20)
plt.yticks(fontsize = 25)
plt.xticks(fontsize = 20)
plt.gcf().subplots_adjust(left=0.15)

visualizer = FreqDistVisualizer(features = features, n=10, orient='h')
visualizer.fit(docs)
visualizer.show()
fig.savefig('./images/freqdistofwordsinplotsummaryforromancegenre.png');

"""The wordcloud below shows the same thing above for romance"""

plot_corpus = ' '.join(romance_plotlist)
plot_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', max_words=20, height=2000, width=4000, random_state = 42, 
                           normalize_plurals = False).generate(plot_corpus)

# Plot the wordcloud
fig, ax = plt.subplots(figsize=(16,8))
plt.imshow(plot_wordcloud)
plt.axis('off')
plt.title('Most Frequent Words in the Movie Plot For Romance', fontsize = 20)
plt.show()
fig.savefig('./images/MoviePlotCloudRomance.png');

"""A worldcloud showing the most frequent words in movie titles for romance"""

title_corpus = ' '.join(romance_plotlist)
title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', height=2000, width=4000, random_state = 42, max_words = 100).generate(title_corpus)

# Plot the wordcloud
fig, ax = plt.subplots(figsize=(16,8))
plt.imshow(title_wordcloud)
plt.axis('off')
plt.title('Most Frequent Words in the Movie Titles For Romance', fontsize = 20)
plt.show()
fig.savefig('./images/MovieTitleCloudRomance.png');

"""# Modeling

## Assigning Independent and Target Variables and performing train test split into training and testing set
"""

X = new_df['Plot']
y = new_df['labels']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state = 42, stratify = y)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

"""## Preprocessing plot column for modeling

## First simple Model using Dummy Classifier

Dummy Classifier just predicting the most frequent class in the dataset gives us a 41% accuracy and we are going to be using that as our baseline accuracy moving forward
"""

dummy_grid = {
    'strategy': ['most_frequent', 'stratified', 'prior', 'uniform', 'constant']
}
dummy = DummyClassifier( random_state = 42)
dummy_gs = GridSearchCV(dummy, param_grid = dummy_grid, cv = 5)
dummy_gs.fit(X_train, y_train)
print(dummy_gs.best_score_)

"""## Using TfidfVectorizer on the independent variable and testing out the Multinomial Naive Bayes Model, Logistic Regression Model  and Decision Tree"""

vectorizer = TfidfVectorizer(ngram_range = (1,1), max_df=.85, min_df=15, lowercase=False)
tfidf_Xtr = vectorizer.fit_transform(X_train)

"""### Grid Search with Multinomial Naive Bayes gave accuracy of ~62%"""

model_resultstfidf = {}
mnb_grid = {
    'alpha': [0, 0.2, 0.5, 0.8, 1.0],
    'fit_prior' : [True, False],
    'class_prior' : [None, [.4, .3, 0.08, .07, .06, .06], [.167, .167, .167, .167, .167, .167]]
}
mnb = MultinomialNB()
mnb_tfidf_gs = GridSearchCV(mnb, param_grid = mnb_grid, cv = 5)
mnb_tfidf_gs.fit(tfidf_Xtr, y_train)
print(mnb_tfidf_gs.best_score_)

model_resultstfidf['Multinomial Bayes'] = mnb_tfidf_gs.best_score_

"""### Grid Search with Logistic Regression gave accuracy of ~64%"""

lr_grid = {
    'penalty' : ['l1', 'l2', 'none'],
    'C' : [1.0, 1e2, 1e4, 1e10],
    'class_weight' : ['balanced', None],
    'solver' : ['lbfgs', 'liblinear'],
    'multi_class' : ['ovr', 'multinomial']
}
logreg = LogisticRegression(random_state = 42)
logreg_tfidf_gs = GridSearchCV(logreg, param_grid = lr_grid, cv = 5)
logreg_tfidf_gs.fit(tfidf_Xtr, y_train)
print(logreg_tfidf_gs.best_score_)
model_resultstfidf['Logistic Regression'] = logreg_tfidf_gs.best_score_

"""### Grid Search with Decision Tree gave accuracy of ~47%"""

grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [1, 2, 5, 10],
    'min_samples_split': [1, 5, 10, 20]
}

dt_clf = DecisionTreeClassifier (random_state = 42)
gs_tfidf_tree = GridSearchCV(dt_clf, param_grid = grid, cv = 5)
gs_tfidf_tree.fit(tfidf_Xtr, y_train)

print(gs_tfidf_tree.best_score_)
model_resultstfidf['Decision Tree'] = gs_tfidf_tree.best_score_

"""## Using Count Vectorizer on the independent variable with Multinomial Naive Bayes Model, Logistic Regression Model  and Decision Tree"""

countvec = CountVectorizer(ngram_range = (1,3), max_features = 30000)
vec_Xtr = countvec.fit_transform(X_train)

"""### Grid Search with Multinomial Naive Bayes gave accuracy of ~60%"""

model_results_cvec = {}
mnb_grid = {
    'alpha': [0, 0.2, 0.5, 0.8, 1.0],
    'fit_prior' : [True, False],
    'class_prior' : [None, [.4, .3, 0.08, .07, .06, .06], [.167, .167, .167, .167, .167, .167]]
}
mnb = MultinomialNB()
mnb_cvec_gs = GridSearchCV(mnb, param_grid = mnb_grid, cv = 5)
mnb_cvec_gs.fit(vec_Xtr, y_train)
print(mnb_cvec_gs.best_score_)
model_results_cvec['Multinomial Bayes'] = mnb_cvec_gs.best_score_

"""### Grid Search with Logistic Regression gave accuracy of ~62%"""

lr_grid = {
    'penalty' : ['l1', 'l2', 'none'],
    'C' : [1.0, 1e2, 1e4, 1e10],
    'class_weight' : ['balanced', None],
    'solver' : ['lbfgs', 'liblinear'],
    'multi_class' : ['ovr', 'multinomial']
}
logreg = LogisticRegression(random_state = 42)
logreg_cvec_gs = GridSearchCV(logreg, param_grid = lr_grid, cv = 5)
logreg_cvec_gs.fit(vec_Xtr, y_train)
print(logreg_cvec_gs.best_score_)
model_results_cvec['Logistic Regression'] = logreg_cvec_gs.best_score_

"""### Grid Search with Decision Tree gave accuracy of ~47%"""

grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [1, 2, 5, 10],
    'min_samples_split': [1, 5, 10, 20]
}

dt_clf = DecisionTreeClassifier (random_state = 42)
gs_cvec_tree = GridSearchCV(dt_clf, param_grid = grid, cv = 5)
gs_cvec_tree.fit(vec_Xtr, y_train)

print(gs_cvec_tree.best_score_)
model_results_cvec['Decision Tree'] = gs_cvec_tree.best_score_

"""## SGD Classifier on count_vectorizer-transformed X variable"""

from sklearn.linear_model import SGDClassifier

grid = {
    'loss' : ['hinge', 'log'],
    'penalty' : ['l2', 'l1'],
    'alpha' : [1e-3, 1e-4, 1e-5],
    'max_iter' : [10, 15, 20],
    'class_weight' : ['balanced', None]
}
sgd = SGDClassifier(random_state=42)
sgd_cvec_gs = GridSearchCV(sgd, param_grid = grid, cv=5)
sgd_cvec_gs.fit(vec_Xtr, y_train)

print(sgd_cvec_gs.best_score_)
model_results_cvec['SGD'] = sgd_cvec_gs.best_score_

sgd_cvec_gs.best_params_

"""# SGD Classifier on tfidf-transformed X variable"""

grid = {
    'loss' : ['hinge', 'log'],
    'penalty' : ['l2', 'l1'],
    'alpha' : [1e-3, 1e-4, 1e-5],
    'max_iter' : [20, 25, 30],
    'class_weight' : ['balanced', None]
}
sgd = SGDClassifier(random_state=42)
sgd_tfidf_gs = GridSearchCV(sgd, param_grid = grid, cv=5)
sgd_tfidf_gs.fit(tfidf_Xtr, y_train)

print(sgd_tfidf_gs.best_score_)
model_resultstfidf['SGD'] = sgd_tfidf_gs.best_score_

sgd_tfidf_gs.best_params_

"""### Comparison of the Different Models and their Scores

The best model is the one that gave us the best score above which is the logistic regression model (logreg_tfidf_gs)on the tfidf-transformed X variable which gave us an accuracy score of approximately 63.3%. Using the '.best_params_' attribute of GridSearchCV, I will obtain the optimal hyparameter values and use it in evaluating the test set.
"""

tfidf_model_results = pd.DataFrame(model_resultstfidf.items(), columns=['model', 'accuracy_score'])
count_vec_results = pd.DataFrame(model_results_cvec.items(), columns=['model', 'accuracy_score'])

y1 = tfidf_model_results['accuracy_score']
y2 = count_vec_results['accuracy_score']

fig, ax = plt.subplots(figsize = (10,8))
ind = np.arange(len(tfidf_model_results))

width = 0.3  
plt.bar(ind, y1, width, color = 'orange')
plt.bar(ind - width, y2, width,color = 'green')

plt.title('Model Comparison')
plt.xlabel('model')
plt.ylabel('accuracy_score')
plt.xticks(np.arange(4), ['multinomial bayes', 'logistic regression', 'decision tree', 'SGD'])
plt.legend(['tfidf_model_results', 'count_vec_results'])
fig.savefig('./images/modelcomparisonplot.png')

"""### Merging the two tables for use in slides"""

model_scores = pd.merge(tfidf_model_results, count_vec_results, how = 'left', on = 'model')
model_scores = model_scores.rename(columns = {'accuracy_score_x':'tfidf_score', 'accuracy_score_y':'count_vec_score'})
dfi.export(model_scores, './images/model_scores.png')

"""## Extracting best Parameters in the best model"""

logreg_tfidf_gs.best_params_

"""# Applying Logistic Regression Model Pipeline to the test set"""

logreg_pipeline = Pipeline([('vect', TfidfVectorizer(ngram_range = (1,1), max_df=.85, min_df=15)),
                            ('clf', LogisticRegression(C = 1.0, class_weight = 'balanced', multi_class = 'ovr', penalty = 'l2', solver = 'liblinear', random_state = 42))])

logreg_pipeline.fit(X_train, y_train)
y_hat_prob=logreg_pipeline.predict_proba(X_test)
y_hat = logreg_pipeline.predict(X_test)
print('Test Accuracy score:', accuracy_score(y_test, y_hat))
print(classification_report(y_test, y_hat))

"""## Logreg Confusion Matrix for Test Set"""

fig, ax = plt.subplots(figsize=(12,10))
plot_confusion_matrix(logreg_pipeline, X_test, y_test, ax = ax, display_labels = ['drama', 'comedy', 'horror', 'action', 'thriller', 'romance'])
plt.grid(None)
plt.xlabel('Predicted Label', size = 15)
plt.ylabel('True Label', size = 15)
plt.xticks(size = 20)
plt.yticks(size = 20)
fig.savefig('./images/confusionmatrix.png')

"""# Applying SGD Model Pipeline to the test set because the cv scores were so close to those of Logistic regression

There is a small decrease in accuracy score by .02%, however it was still pretty good. I will be sticking to the Logistic Regression becasue that is best overall especially when comparing the confusion matrix
"""

sgd_pipeline = Pipeline([('vect', TfidfVectorizer(ngram_range = (1,1), max_df=.85, min_df=15)),
                            ('clf', SGDClassifier(alpha = 0.0001, loss = 'log', max_iter = 20, penalty = 'l2', class_weight = 'balanced', random_state = 42))])

sgd_pipeline.fit(X_train, y_train)
y_hat_sgd = sgd_pipeline.predict(X_test)
print('Test Accuracy score:', accuracy_score(y_test, y_hat_sgd))
print(classification_report(y_test, y_hat_sgd))

""" ## SGD Confusion Matrix for Test Set"""

fig, ax = plt.subplots(figsize=(12,10))
plot_confusion_matrix(sgd_pipeline, X_test, y_test, ax = ax, display_labels = ['drama', 'comedy', 'horror', 'action', 'thriller', 'romance'])
plt.grid(None)
plt.xlabel('Predicted Label', size = 15)
plt.ylabel('True Label', size = 15)
plt.xticks(size = 20)
plt.yticks(size = 20)
# fig.savefig('./images/confusionmatrixsgd.png')

"""### Calculating cosine similarity between thegenres based on the words"""

genre_plotlist = [['drama', ' '.join(list(set(drama_plotlist)))], ['comedy', ' '.join(list(set(comedy_plotlist)))], 
                  ['horror', ' '.join(list(set(horror_plotlist)))], ['action', ' '.join(list(set(action_plotlist)))],
                 ['thriller', ' '.join(list(set(thriller_plotlist)))], ['romance', ' '.join(list(set(romance_plotlist)))]]
genre_plot_df = pd.DataFrame(genre_plotlist, columns = ['genre_type', 'plot_words'])
genre_plot_df

"""### The genres have a high cosine similarity with each other (0.46 and above)"""

from sklearn.metrics.pairwise import cosine_similarity

count_vec = CountVectorizer()
count_matrix = count_vec.fit_transform(genre_plot_df['plot_words'])
cosine_sim = pd.DataFrame(cosine_similarity(count_matrix))
cosine_sim

"""## Results

- Baseline accuracy was 41%
- The tfidf transformed plots performed better during modeling. 
- GridSearchCV helped in narrowing down the best model hyperparameter values.
- The model with the best performance was the Logistic Regression Model on tfidf transformed plots with the following parameters (although SGD ranked pretty highly as well):
   - C = 1.0
   - class_weight = 'balanced'
   - multi_class = 'ovr'
   - penalty = 'l2'
   - solver = 'liblinear'
- When the above model was used on the test set, it produced an accuracy score of ~64% which a significant increase from the baseline
- The confusion matrix confirms what I suspected when looking at the most common words in the genres. 
- Due to the fact that some of the genres had words in common with another genre, false predictions of those genres were mostly as the genres they had common words with.
- The decision tree models performed the least favorably.
"""